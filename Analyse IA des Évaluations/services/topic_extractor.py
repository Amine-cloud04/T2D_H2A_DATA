import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter
import re
from typing import List, Dict, Tuple
import subprocess
import sys

class TopicExtractor:
    """Extrait les th√®mes principaux des commentaires"""
    
    def __init__(self, language: str = "fr"):
        print("üîç Loading spaCy model for topic extraction...")
        self.nlp = None
        
        try:
            if language == "fr":
                self.nlp = spacy.load("fr_core_news_sm")
                print("‚úÖ spaCy French model loaded successfully")
        except OSError:
            print("‚ö†Ô∏è  spaCy model not found, downloading...")
            try:
                subprocess.check_call([
                    sys.executable, "-m", "spacy", "download", "fr_core_news_sm"
                ])
                self.nlp = spacy.load("fr_core_news_sm")
                print("‚úÖ spaCy model downloaded and loaded")
            except Exception as e:
                print(f"‚ùå Could not download spaCy model: {e}")
                print("‚ö†Ô∏è  Topic extraction will use basic text processing")
                self.nlp = None
        
        # ‚úÖ LISTE √âTENDUE DE STOP WORDS PERSONNALIS√âS
        self.custom_stopwords = {
            # Mots g√©n√©riques de formation
            'formation', 'formateur', 'formatrice', 'session', 'jour', 'jours',
            'cours', 'module', 'participant', 'participants', 'groupe',
            
            # Adverbes et conjonctions
            'tr√®s', 'bien', 'bon', 'bonne', 'mauvais', 'mauvaise', 'merci', 'etc',
            'plus', 'moins', 'faire', 'avoir', '√™tre', 'peut', 'pouvoir',
            'aussi', 'encore', 'toujours', 'jamais', 'beaucoup', 'peu',
            'assez', 'trop', 'pas', 'sans', 'avec', 'mais', 'donc', 'car',
            
            # Mots vagues
            'correct', 'moyen', 'moyenne', 'rien', 'aucun', 'aucune',
            'ni', 'quelque', 'plusieurs', 'certain', 'certaine',
            
            # N√©gations et particules
            'ne', 'pas', 'non', 'aucunement', 'nullement',
            
            # Expressions communes
            'de', 'du', 'des', 'un', 'une', 'le', 'la', 'les',
            
            # Mots de liaison
            'pour', 'par', 'sur', 'dans', 'sous', 'vers', 'chez',
            'au', 'aux', '√†', 'en', 'depuis', 'pendant',
            
            # Mots g√©n√©riques de feedback
            'int√©ressant', 'int√©ressante', 'interessant', 'interessante'
        }
        
        # ‚úÖ EXPRESSIONS √Ä SUPPRIMER (bigrammes/trigrammes non informatifs)
        self.stopword_phrases = {
            'pas assez', 'trop de', 'manque de', 'assez rien',
            'ni ni', 'sans sans', 'mais mais', 'correct correct'
        }
    
    def preprocess_for_topics(self, texts: List[str]) -> List[str]:
        """Pr√©traite les textes pour l'extraction de th√®mes"""
        processed = []
        
        for text in texts:
            if not text or len(text.strip()) < 10:
                continue
            
            if self.nlp:
                # Analyse avec spaCy
                doc = self.nlp(text.lower())
                
                # ‚úÖ FILTRAGE AM√âLIOR√â
                tokens = []
                for token in doc:
                    # Conditions strictes pour garder un token
                    if (not token.is_stop 
                        and not token.is_punct 
                        and not token.is_space
                        and token.pos_ in ['NOUN', 'ADJ', 'VERB']  # Seulement noms, adjectifs, verbes
                        and len(token.text) > 3  # Minimum 4 caract√®res
                        and token.lemma_ not in self.custom_stopwords
                        and token.text not in self.custom_stopwords
                        and not token.is_digit  # Pas de chiffres
                        and token.text.isalpha()):  # Seulement des lettres
                        
                        tokens.append(token.lemma_)
            else:
                # Fallback : traitement basique
                tokens = self._basic_tokenize(text.lower())
            
            if tokens and len(tokens) >= 2:  # Au moins 2 tokens significatifs
                processed.append(' '.join(tokens))
        
        return processed
    
    def _basic_tokenize(self, text: str) -> List[str]:
        """Tokenisation basique am√©lior√©e sans spaCy"""
        # Supprimer la ponctuation
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # S√©parer en mots
        words = text.split()
        
        # ‚úÖ STOP WORDS FRAN√áAIS COMPLETS
        french_stopwords = {
            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du', 'd',
            'et', 'ou', 'mais', 'donc', 'or', 'ni', 'car',
            'ce', 'cet', 'cette', 'ces', 'mon', 'ton', 'son', 'ma', 'ta', 'sa',
            'mes', 'tes', 'ses', 'notre', 'votre', 'leur', 'nos', 'vos', 'leurs',
            'je', 'tu', 'il', 'elle', 'on', 'nous', 'vous', 'ils', 'elles',
            '√†', 'au', 'aux', 'en', 'sur', 'pour', 'par', 'dans',
            'avec', 'sans', 'sous', 'vers', 'chez', 'depuis', 'pendant',
            '√™tre', 'avoir', 'faire', 'dire', 'aller', 'voir', 'savoir',
            'pouvoir', 'falloir', 'vouloir', 'devoir', 'croire',
            'tr√®s', 'plus', 'moins', 'aussi', 'trop', 'assez', 'peu',
            'bien', 'mal', 'mieux', 'pire', 'beaucoup', 'pas', 'ne',
            'oui', 'non', 'si', 'comment', 'pourquoi', 'quand', 'o√π',
            'tout', 'tous', 'toute', 'toutes', 'autre', 'm√™me', 'tel',
            'quel', 'quelle', 'quels', 'quelles', 'quelque', 'quelques',
            'chaque', 'certain', 'certaine', 'certains', 'certaines',
            'plusieurs', 'aucun', 'aucune', 'nul', 'nulle'
        }
        
        tokens = []
        for word in words:
            # Filtrage strict
            if (len(word) > 3  # Minimum 4 caract√®res
                and word not in french_stopwords
                and word not in self.custom_stopwords
                and word.isalpha()  # Seulement lettres
                and not word.isdigit()):  # Pas de chiffres
                tokens.append(word)
        
        return tokens
    
    def _remove_stopword_phrases(self, text: str) -> str:
        """Supprime les expressions non informatives"""
        for phrase in self.stopword_phrases:
            text = text.replace(phrase, '')
        return text
    
    def extract_keywords_tfidf(self, texts: List[str], top_n: int = 20) -> List[Tuple[str, float]]:
        """Extrait les mots-cl√©s avec TF-IDF am√©lior√©"""
        if not texts or len(texts) < 2:
            print("‚ö†Ô∏è  Not enough texts for TF-IDF")
            return []
        
        processed_texts = self.preprocess_for_topics(texts)
        
        if len(processed_texts) < 2:
            print("‚ö†Ô∏è  Not enough processed texts")
            return []
        
        try:
            # ‚úÖ TF-IDF AM√âLIOR√â
            vectorizer = TfidfVectorizer(
                max_features=150,
                ngram_range=(1, 3),  # Unigrammes, bigrammes ET trigrammes
                min_df=3,  # Doit appara√Ætre dans au moins 3 documents
                max_df=0.7,  # Maximum 70% des documents (pas trop fr√©quent)
                token_pattern=r'\b[a-z√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß]{4,}\b',  # Minimum 4 lettres
                strip_accents='unicode'
            )
            
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calculer les scores moyens
            scores = tfidf_matrix.mean(axis=0).A1
            
            # Filtrer les r√©sultats
            filtered_keywords = []
            for i, score in enumerate(scores):
                keyword = feature_names[i]
                
                # ‚úÖ FILTRAGE SUPPL√âMENTAIRE
                # V√©rifier que le mot-cl√© n'est pas dans les stopwords
                if (keyword not in self.custom_stopwords 
                    and not any(stop in keyword for stop in self.custom_stopwords)
                    and score > 0.01):  # Score minimum
                    filtered_keywords.append((keyword, score))
            
            # Trier et prendre les top N
            filtered_keywords.sort(key=lambda x: x[1], reverse=True)
            keywords = filtered_keywords[:top_n]
            
            print(f"‚úÖ Extracted {len(keywords)} meaningful keywords")
            return keywords
        
        except Exception as e:
            print(f"‚ùå TF-IDF extraction error: {e}")
            return []
    
    def extract_themes_lda(self, texts: List[str], n_topics: int = 5) -> List[Dict]:
        """Extrait les th√®mes avec LDA am√©lior√©"""
        processed_texts = self.preprocess_for_topics(texts)
        
        if len(processed_texts) < 10:
            print("‚ö†Ô∏è  Not enough texts for LDA, using keywords instead")
            keywords = self.extract_keywords_tfidf(texts, top_n=10)
            return [{
                'theme': kw[0],
                'keywords': [kw[0]],
                'weight': kw[1]
            } for kw in keywords[:5]]
        
        try:
            # ‚úÖ VECTORISATION AM√âLIOR√âE
            vectorizer = TfidfVectorizer(
                max_features=150,
                min_df=3,
                max_df=0.7,
                ngram_range=(1, 2),
                token_pattern=r'\b[a-z√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß]{4,}\b'
            )
            
            doc_term_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # ‚úÖ LDA AM√âLIOR√â
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=30,  # Plus d'it√©rations
                learning_method='batch',
                n_jobs=-1  # Utiliser tous les CPU
            )
            
            lda.fit(doc_term_matrix)
            
            # Extraire les th√®mes
            themes = []
            for topic_idx, topic in enumerate(lda.components_):
                top_indices = topic.argsort()[-8:][::-1]  # Top 8 mots
                top_words = [feature_names[i] for i in top_indices]
                
                # ‚úÖ FILTRER LES MOTS NON SIGNIFICATIFS
                meaningful_words = [
                    word for word in top_words 
                    if word not in self.custom_stopwords
                ][:5]
                
                if len(meaningful_words) >= 2:
                    # Cr√©er un nom de th√®me bas√© sur les 2-3 mots principaux
                    theme_name = ' + '.join(meaningful_words[:3])
                    
                    themes.append({
                        'theme': theme_name,
                        'keywords': meaningful_words,
                        'weight': float(topic.sum())
                    })
            
            print(f"‚úÖ Extracted {len(themes)} meaningful themes with LDA")
            return themes
        
        except Exception as e:
            print(f"‚ùå LDA extraction error: {e}")
            return []
    
    def extract_frequent_phrases(self, texts: List[str], min_count: int = 3) -> List[Tuple[str, int]]:
        """Extrait les phrases fr√©quentes significatives"""
        all_ngrams = []
        
        for text in texts:
            if self.nlp:
                doc = self.nlp(text.lower())
                
                # ‚úÖ EXTRACTION AM√âLIOR√âE DE BIGRAMMES
                for i in range(len(doc) - 1):
                    token1, token2 = doc[i], doc[i + 1]
                    
                    # Conditions strictes
                    if (not token1.is_stop and not token2.is_stop
                        and token1.pos_ in ['NOUN', 'ADJ', 'VERB'] 
                        and token2.pos_ in ['NOUN', 'ADJ', 'VERB']
                        and len(token1.text) > 3 and len(token2.text) > 3
                        and token1.lemma_ not in self.custom_stopwords
                        and token2.lemma_ not in self.custom_stopwords):
                        
                        bigram = f"{token1.lemma_} {token2.lemma_}"
                        
                        # V√©rifier que ce n'est pas une expression stopword
                        if bigram not in self.stopword_phrases:
                            all_ngrams.append(bigram)
            else:
                # Fallback basique
                words = self._basic_tokenize(text.lower())
                for i in range(len(words) - 1):
                    bigram = f"{words[i]} {words[i + 1]}"
                    if bigram not in self.stopword_phrases:
                        all_ngrams.append(bigram)
        
        # Compter et filtrer
        counter = Counter(all_ngrams)
        frequent = [
            (phrase, count) for phrase, count in counter.most_common(30) 
            if count >= min_count and len(phrase.split()) == 2  # Seulement bigrammes complets
        ][:20]
        
        return frequent


